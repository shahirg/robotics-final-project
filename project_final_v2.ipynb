{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_color = .8\n",
    "current_color = .5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "actions = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down'\n",
    "}\n",
    "\n",
    "num_actions = len(actions)\n",
    "\n",
    "epsilon = .15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(object):\n",
    "    def __init__(self, maze, curr=(0,0)) -> None:\n",
    "        self._maze = np.array(maze)\n",
    "        num_rows, num_cols = self._maze.shape\n",
    "        self.target = (num_rows - 1, num_cols - 1) # end goal\n",
    "\n",
    "        self.free_cells = []\n",
    "        for r in range(num_rows):\n",
    "            for c in range(num_cols):\n",
    "                if self._maze[r,c] == 1:\n",
    "                    self.free_cells.append((r,c))\n",
    "        self.free_cells.remove(self.target)\n",
    "        self.reset(curr)\n",
    "    \n",
    "    def reset(self, curr):\n",
    "        self.curr = curr\n",
    "        # use a copy of original maze\n",
    "        self.maze = np.copy(self._maze)\n",
    "        num_rows, num_cols = self._maze.shape\n",
    "        row, col = curr\n",
    "        self.maze[row, col] = current_color\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        num_rows, num_cols = self._maze.shape\n",
    "        nrow, ncol, nmode = curr_row, curr_col, mode = self.state\n",
    "        \n",
    "        if self.maze[curr_row, curr_col] > 0:\n",
    "            self.visited.add((curr_row,curr_col))\n",
    "        \n",
    "        valid_actions = self.valid_actions()\n",
    "\n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # set new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        curr_row, curr_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if curr_row == nrows - 1 and curr_col == ncols - 1:\n",
    "            return 1\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (curr_row, curr_col) in self.visited:\n",
    "            return -.25\n",
    "        if mode == 'invalid':\n",
    "            return -.75\n",
    "        if mode == 'valid':\n",
    "            return -.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the person\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = current_color\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        curr_row, curr_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if curr_row == nrows-1 and curr_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "    \n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "\n",
    "        # check if cant move up\n",
    "        if row == 0 or (row > 0 and self.maze[row - 1, col] == 0):\n",
    "            actions.remove(1)\n",
    "\n",
    "        # check if cant move down\n",
    "        if row == nrows - 1 or (row < nrows - 1 and self.maze[row + 1, col] == 0):\n",
    "            actions.remove(3)\n",
    "\n",
    "        # check if cant move left\n",
    "        if col == 0 or (col > 0 and self.maze[row, col - 1] == 0):\n",
    "            actions.remove(0)\n",
    "\n",
    "        # check if cant move right\n",
    "        if col == ncols - 1 or (col<ncols - 1 and self.maze[row, col + 1] == 0):\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(maze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = maze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(maze.maze)\n",
    "    for row,col in maze.visited:\n",
    "        canvas[row,col] = visited_color\n",
    "    curr_row, curr_col, _ = maze.state\n",
    "    canvas[curr_row, curr_col] = current_color   # current cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # target cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= -0.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2aaf745b6d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze = [\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "]\n",
    "\n",
    "maze = Maze(maze)\n",
    "canvas, reward, game_over = maze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2aaf9588e50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze.act(DOWN)  # move down\n",
    "maze.act(RIGHT)  # move right\n",
    "maze.act(RIGHT)  # move right\n",
    "maze.act(RIGHT)  # move right\n",
    "maze.act(UP)  # move up\n",
    "show(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, maze, curr_cell):\n",
    "    maze.reset(curr_cell)\n",
    "    envstate = maze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = maze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, maze, epochs=1000, max_memory=1000, data_size=50, weights_file='', name='model_data'):\n",
    "    global epsilon\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Maze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    # lists for plotting\n",
    "    episodes_list = []\n",
    "    loss_list = []\n",
    "    win_count_list = []\n",
    "    win_rate_list = []\n",
    "    time_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = datetime.datetime.now()\n",
    "        loss = 0.0\n",
    "        curr_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(curr_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        epoch_time = datetime.datetime.now() - epoch_start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, epochs-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        \n",
    "        loss_list.append(loss)\n",
    "        episodes_list.append(n_episodes)\n",
    "        win_count_list.append(sum(win_history))\n",
    "        win_rate_list.append(win_rate)\n",
    "        time_list.append(epoch_time)\n",
    "\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"epochs: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2aaf95f8760>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOsAAADrCAYAAACICmHVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAFQ0lEQVR4nO3dMW5TaRiF4f+OkJAgI1KMdCWU3untBYRVsAPYwG0RGzA1EivILuIFxAVlOgokFCllqP8pZooZYUgsQj5O7vNIrgI6F+IX4uobeu8N+P39Uf0AwO2IFUKIFUKIFUKIFUKIFUI82ucXP3nypB8eHv6iR7nZly9fSnaPj4/b06dPS7a/fv1qe0bbnz59aldXV8Our+0V6+HhYXv9+vXdPNWeDg4O2jRNJdvv379vJycnJdubzcb2jLZXq9V3v+bHYAghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgghVgix12Gq58+ft7dv3/6qZ/mhzWbTeu9l21W222178eJFyfZ6vS7drjoO1Vprw7DzkFup4aYAhmF41Vp71Vpr4zguT09P7+O5vnF9fd0ODg5mt315edk+f/5csn10dFS6PY5jyfb19XW7uLgo2Z6mqfXed/9L0Xu/9Wu5XPYqZ2dns9xer9e9tVbyqt6ucnZ2Vvbn/ifJ3f35zAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAohxAoh9op1u922YRhKXnPdXi6Xex0Pu8tX9Tb/t9fJx2fPni3fvHlzH8/1jerzg1Xbi8Vilqcuq7fjTz62wjN41ecHq7bneuqyervyvd6dfIRsYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQYoUQe8VafQJwjtvV5nhmc7vdlr7Xvvu9uOkN8d+Tj+M4Lk9PT+/0zXBb1ScA57pddfqw+sTnOI4l29M0tfPz858/+bhcLnuV6hOAc91uMzyzuV6vy/7O/21sZ38+s0IIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUKImJOPl5eXpScA57pddfqw+tRl1faDOPlYfQJwrttVqk9dVnHyER4AsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIsUIIJx9vYbFYzPL8oO375+TjT77men7Q9v1z8hEeALFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCCLFCiJiTj3M9AVh56vLo6KiN41iyXf39fvz4ccn2NE3t48ePO08+PrrpN/feP7TWPrTW2mq16icnJ3f7dLe02WzaHLffvXvXpmkq2V6v1+3ly5cl29Xf78ViUbL9I34MhhBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRBihRB7nXxsrS1aaxe/+qG+46/W2pVt2w98e9F7/3PXF26M9XcxDMN5731l2/Zct/0YDCHECiGSYv1g2/act2M+s8LcJf3PCrMmVgghVgghVgghVgjxN9IwkgpigMs/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])\n",
    "\n",
    "qmaze = Maze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(list, title='', x_label='', y_label=''):\n",
    "    f, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,5))\n",
    "    f.suptitle(title)\n",
    "    ax.plot(list, label='loss')\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    x = range(len(list))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/999 | Loss: 0.0166 | Episodes: 13 | Win count: 1 | Win rate: 0.000 | time: 13.0 seconds\n"
     ]
    }
   ],
   "source": [
    "model = build_model(maze)\n",
    "qtrain(model, maze, epochs=1000, max_memory=8*maze.size, data_size=32)\n",
    "\n",
    "epochs=1000\n",
    "max_memory=8*maze.size \n",
    "data_size=50\n",
    "weights_file=''\n",
    "name='model_data21'\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# If you want to continue training from a previous model,\n",
    "# just supply the h5 file name to weights_file option\n",
    "if weights_file:\n",
    "    print(\"loading weights from file: %s\" % (weights_file,))\n",
    "    model.load_weights(weights_file)\n",
    "\n",
    "# Construct environment/game from numpy array: maze (see above)\n",
    "qmaze = Maze(maze)\n",
    "\n",
    "# Initialize experience replay object\n",
    "experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "win_history = []   # history of win/lose game\n",
    "n_free_cells = len(qmaze.free_cells)\n",
    "hsize = qmaze.maze.size//2   # history window size\n",
    "win_rate = 0.0\n",
    "imctr = 1\n",
    "\n",
    "# lists for plotting\n",
    "episodes_list = []\n",
    "loss_list = []\n",
    "games_list = []\n",
    "win_count_list = []\n",
    "win_rate_list = []\n",
    "time_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = datetime.datetime.now()\n",
    "    loss = 0.0\n",
    "    curr_cell = random.choice(qmaze.free_cells)\n",
    "    qmaze.reset(curr_cell)\n",
    "    game_over = False\n",
    "\n",
    "    # get initial envstate (1d flattened canvas)\n",
    "    envstate = qmaze.observe()\n",
    "\n",
    "    n_episodes = 0\n",
    "    while not game_over:\n",
    "        valid_actions = qmaze.valid_actions()\n",
    "        if not valid_actions: break\n",
    "        prev_envstate = envstate\n",
    "        # Get next action\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = random.choice(valid_actions)\n",
    "        else:\n",
    "            action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "        # Apply action, get reward and new envstate\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            win_history.append(1)\n",
    "            game_over = True\n",
    "        elif game_status == 'lose':\n",
    "            win_history.append(0)\n",
    "            game_over = True\n",
    "        else:\n",
    "            game_over = False\n",
    "\n",
    "        # Store episode (experience)\n",
    "        episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "        experience.remember(episode)\n",
    "        n_episodes += 1\n",
    "\n",
    "        # Train neural network model\n",
    "        inputs, targets = experience.get_data(data_size=data_size)\n",
    "        h = model.fit(\n",
    "            inputs,\n",
    "            targets,\n",
    "            epochs=8,\n",
    "            batch_size=16,\n",
    "            verbose=0,\n",
    "        )\n",
    "        loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "    if len(win_history) > hsize:\n",
    "        win_rate = sum(win_history[-hsize:]) / hsize\n",
    "\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    epoch_time = datetime.datetime.now() - epoch_start_time\n",
    "    t = format_time(dt.total_seconds())\n",
    "    template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "    print(template.format(epoch, epochs-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "    \n",
    "    loss_list.append(loss)\n",
    "    episodes_list.append(n_episodes)\n",
    "    games_list.append(len(win_history))\n",
    "    win_count_list.append(sum(win_history))\n",
    "    win_rate_list.append(win_rate)\n",
    "    time_list.append(epoch_time.total_seconds())\n",
    "\n",
    "    # we simply check if training has exhausted all free cells and if in all\n",
    "    # cases the agent won\n",
    "    if win_rate > 0.9 : epsilon = 0.05\n",
    "    if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "        print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "        break\n",
    "\n",
    "# Save trained model weights and architecture, this will be used by the visualization code\n",
    "h5file = name + \".h5\"\n",
    "json_file = name + \".json\"\n",
    "model.save_weights(h5file, overwrite=True)\n",
    "with open(json_file, \"w\") as outfile:\n",
    "    json.dump(model.to_json(), outfile)\n",
    "end_time = datetime.datetime.now()\n",
    "dt = datetime.datetime.now() - start_time\n",
    "seconds = dt.total_seconds()\n",
    "t = format_time(seconds)\n",
    "print('files: %s, %s' % (h5file, json_file))\n",
    "print(\"epochs: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1632/16766045.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# plotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Epoch vs. Episode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Episodes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Epoch vs. Loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgames_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Epoch vs. Games Playedt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Games Played'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "# plotting\n",
    "\n",
    "plot(episodes_list,'Epoch vs. Episode', 'Epoch', 'Episodes')\n",
    "plot(loss_list,'Epoch vs. Loss', 'Epoch', 'Loss')\n",
    "plot(games_list,'Epoch vs. Games Playedt', 'Epoch', 'Games Played')\n",
    "plot(win_count_list,'Epoch vs. Win Count', 'Epoch', 'Win Count')\n",
    "plot(win_rate_list,'Epoch vs. Win Rate', 'Epoch', 'Win Rate')\n",
    "plot(time_list,'Epoch vs. Time', 'Epoch', 'Time')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2751403e1c15b64e5bc1cfae11adf738bf1b3a7123710357ad6ac675119905ee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
